{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction, transformation, and loading into a JSON file\n",
    "This is part of the project described in <https://github.com/amchagas/OSH_papers_DB>, check the project readme for more details.\n",
    "\n",
    "This notebook loads data sources and merges them in a single compressed JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import rispy\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from project_definitions import baseDir, dataSourceDir, dataOutDir, figDir, articleDataFile\n",
    "from project_definitions import store_data, load_data\n",
    "from pprint import pprint\n",
    "import html\n",
    "from urllib.parse import unquote\n",
    "from jellyfish import damerau_levenshtein_distance as edit_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scieloSource = {\n",
    "    'paths': [dataSourceDir / x for x in (\"scielo.ris\",)],\n",
    "    'rispy_args': {},\n",
    "    'col_rename': {},\n",
    "    'transforms': [],\n",
    "}\n",
    "scopusSource = {\n",
    "    'paths': [dataSourceDir / x for x in (\"scopus.ris\",)],\n",
    "    'rispy_args': {},\n",
    "    'col_rename': {},\n",
    "    'transforms': [],\n",
    "}\n",
    "wosSource = {\n",
    "    'paths': [\n",
    "        dataSourceDir / x for x in \"wos1001-1500.ciw  wos1-500.ciw  wos1501-1689.ciw  wos501-1000.ciw\".split()\n",
    "    ],\n",
    "    'rispy_args': {'implementation': 'wok'},\n",
    "    'col_rename': {'publication_year': 'year', 'document_title': 'title'},\n",
    "    'transforms': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source(dataSource):\n",
    "    dfs = []\n",
    "    for path in dataSource['paths']:\n",
    "        with path.open() as f:\n",
    "            df = pd.DataFrame(rispy.load(f, **dataSource['rispy_args']))\n",
    "        df['__source'] = [[path.name] for _ in range(len(df))]\n",
    "        dfs.append(df)\n",
    "    cdf = pd.concat(dfs, join='outer', ignore_index=True)\n",
    "    cdf = cdf.rename(columns=dataSource['col_rename'])\n",
    "    for trans in dataSource['transforms']:\n",
    "        cdf = cdf.transform(trans)\n",
    "    return cdf.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scieloData = load_source(scieloSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopusData = load_source(scopusSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wosData = load_source(wosSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataList = [scieloData, scopusData, wosData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.concat(allDataList, join='outer', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only article data\n",
    "article_data = allData.loc[allData[\"type_of_reference\"].eq('JOUR') | allData[\"publication_type\"].eq('J')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize DOI\n",
    "article_data.loc[:, 'doi'] = article_data['doi'].str.translate(\n",
    "    str.maketrans(string.ascii_lowercase, string.ascii_uppercase)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spurious records\n",
    "article_data = article_data.loc[article_data['url'].ne(\n",
    "    \"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052219975&partnerID=40&md5=7b54756675a6d510c9db069b49b634d6\"\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct faulty records\n",
    "data_corrections = {\n",
    "    'doi': {\n",
    "        r'^(.*)/PDF$': r'\\1',\n",
    "    }\n",
    "}\n",
    "corrected_article_data = article_data.replace(data_corrections, regex=True)\n",
    "article_data.compare(corrected_article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = corrected_article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_series_keep_longest(sx):\n",
    "    if sx.isna().all():\n",
    "        return np.nan\n",
    "    if sx.name == '__source':\n",
    "        return sx.sum()\n",
    "    if sx.name == 'doi':\n",
    "        if len(sx.dropna().unique()) > 1:\n",
    "            print('Warning, merging different DOIs:\\n', sx)\n",
    "            return list(sx.dropna().unique())\n",
    "    return sx[sx.map(len, na_action='ignore').idxmax()] # Keep a list of all DOIs - must explode before using!\n",
    "\n",
    "def merge_records_keep_longest(dfx):\n",
    "    return dfx.agg(merge_series_keep_longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data with same DOI\n",
    "article_doi = article_data.groupby(article_data['doi'].values).agg(merge_records_keep_longest)\n",
    "\n",
    "# Reassemble data with and without DOI\n",
    "article_nodoi = article_data[~article_data.doi.isin(article_doi.index)]\n",
    "article_data = pd.concat([article_doi, article_nodoi], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_titles(sx):\n",
    "    return (\n",
    "        sx\n",
    "        .str.lower()\n",
    "        .str.replace(r'[^\\s\\w]', ' ', regex=True)\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.strip()\n",
    "        # .map(remove_diacritics) # no need as our corpus is in English\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    \"\"\"\n",
    "    Index string values with similar strings under the same index, for use in a `groupby`.\n",
    "\n",
    "    First normalizes titles. Then, for each value, returns the index of the first previously indexed value\n",
    "    whose edit_distance is <= threshold, or a new index if none is found.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, threshold=0):\n",
    "        self.df = df\n",
    "        assert not df['title'].hasnans\n",
    "        self.titles = clean_titles(self.df['title'])\n",
    "        self.threshold = threshold\n",
    "        self.match_index = {}\n",
    "    def match(self, x):\n",
    "        x = self.titles.loc[x]\n",
    "        if x in self.match_index:\n",
    "            return self.match_index[x]\n",
    "        if self.threshold > 0:\n",
    "            for m, idx in self.match_index.items():\n",
    "                if edit_distance(x, m) <= self.threshold:\n",
    "                    self.match_index[x] = idx\n",
    "                    return self.match_index[x]\n",
    "        self.match_index[x] = len(self.match_index)\n",
    "        return self.match_index[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_g = article_data.groupby(Match(article_data, 5).match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = articles_g.agg(list)[articles_g.size() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test alternatives matchers\n",
    "if False:\n",
    "    articles_gx = article_data.groupby(Match(article_data, 15).match)\n",
    "    bb = articles_gx.agg(list)[articles_gx.size() > 1]\n",
    "    pprint([sorted(x) for x in (\n",
    "        set(clean_titles(aa.explode('title')['title'])).difference(clean_titles(bb.explode('title')['title'])),\n",
    "        set(clean_titles(bb.explode('title')['title'])).difference(clean_titles(aa.explode('title')['title'])),\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    return remove_diacritics(name.split(',')[0].split(' ')[-1].lower().replace(' ', '').replace('-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that matching titles also have matching year\n",
    "sel = aa['year'].map(lambda x: len(set(x)) > 1)\n",
    "aa[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that matching titles also have matching author (impl: first author last name)\n",
    "sel = aa['authors'].map(\n",
    "    lambda merged_authors: set(\n",
    "        tuple( # last name of each author\n",
    "            clean_name(author)\n",
    "            for author in authors\n",
    "        )\n",
    "        for authors in merged_authors\n",
    "        if not ( isinstance(authors, float) and pd.isna(authors) ) # skip NANs\n",
    "    )\n",
    ").map(\n",
    "    lambda merged_lastnames: sum(\n",
    "        edit_distance(firstauthor, other_firstauthor) # sum the edit distances\n",
    "        for merged_firstauthor in list(zip(*merged_lastnames))[:1] # first authors\n",
    "        for i, firstauthor in enumerate(merged_firstauthor)\n",
    "        for other_firstauthor in merged_firstauthor[i+1:] # distinct pairs\n",
    "    )\n",
    ") > 0\n",
    "aa[sel].authors.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data[['doi', 'title', 'authors']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_data = articles_g.agg(merge_records_keep_longest)\n",
    "article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store deduplicated data and check the stored version reproduces the data\n",
    "store_data(article_data, articleDataFile)\n",
    "assert article_data.equals(load_data(articleDataFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load article data (if already stored from the code above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = load_data(articleDataFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOS Collection sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosData = pd.read_csv('https://raw.githubusercontent.com/amchagas/open-source-toolkit/main/plos-items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_article = plosData[\n",
    "    \"Content Type (URL items only - Research Article, Web Article, Commentary, Video, Poster)\"\n",
    "].eq(\"Research Article\")\n",
    "sel_hardware = plosData[\"Hardware or software\"].eq(\"hardware\")\n",
    "plosData = plosData.loc[sel_article & sel_hardware]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert plosData[\"URI (DOI or URL)\"].notna().all()\n",
    "# Normalize DOI\n",
    "plosData[\"URI (DOI or URL)\"] = plosData[\"URI (DOI or URL)\"].str.translate(\n",
    "    str.maketrans(string.ascii_lowercase, string.ascii_uppercase)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the doi and doi-like, fixing doi-like containing extra stuff\n",
    "re_doi = r\"(10\\.[1-9]\\d{3,}(?:\\.\\d+)*/.+)\"\n",
    "re_http_doi_fix = r\"HTTPS?://.*/\" + re_doi + r\"(?:/|/FULL|/ABSTRACT|#\\w+)$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosData_doi = plosData['URI (DOI or URL)'].str.extract(re_doi)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosData_doi_http_doi_fixed = (\n",
    "    plosData['URI (DOI or URL)']\n",
    "    .str.extract(re_http_doi_fix)[0]\n",
    "    .map(unquote, na_action='ignore')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosData_doi.loc[plosData_doi_http_doi_fixed.notna()].compare(plosData_doi_http_doi_fixed.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'doi' not in plosData\n",
    "plosData['doi'] = plosData_doi_http_doi_fixed.where(plosData_doi_http_doi_fixed.notna(), plosData_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosData['doi'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(set(plosData['doi'].dropna()).intersection(article_data['doi'].explode())),\n",
    "    len(set(plosData['doi'].dropna()).symmetric_difference(article_data['doi'].explode())),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosData['Title (URL items only)'] = plosData['Title (URL items only)'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many from the collection have their title in article_data\n",
    "plosData['Title (URL items only)'].pipe(clean_titles).map(\n",
    "    lambda x: article_data['title'].pipe(clean_titles).str.contains(rf'(?i){x}', regex=True).any()\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many from the collection have their title in article_data if we require they have DOIs\n",
    "plosData['Title (URL items only)'].loc[plosData['doi'].notna()].pipe(clean_titles).map(\n",
    "    lambda x: article_data.loc[article_data['doi'].notna()].title.pipe(clean_titles).str.contains(rf'(?i){x}', regex=True).any()\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me 10 from the collection having DOIs\n",
    "z = plosData['doi'].dropna().sample(10)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get their titles if their titles are not in article_data\n",
    "for i, title in plosData.loc[z.index]['Title (URL items only)'].pipe(clean_titles).items():\n",
    "    if not clean_titles(article_data['title']).str.contains(rf'(?i){title}', regex=True).any():\n",
    "        print(i, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector for DOIs only in the collection\n",
    "sel_new_doi = ~plosData[\"doi\"].dropna().isin(article_data['doi'].explode().values)\n",
    "sel_new_doi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector for Titles only in the collection\n",
    "sel_new_title = ~clean_titles(plosData[\"Title (URL items only)\"]).isin(clean_titles(article_data['title']))\n",
    "sel_new_title.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same title, different DOIs\n",
    "x = plosData[[\"doi\", \"Title (URL items only)\"]].loc[sel_new_doi & ~sel_new_title]\n",
    "for i, y in x[\"Title (URL items only)\"].str.lower().items():\n",
    "    print(\n",
    "        y,\n",
    "        article_data[\"doi\"].loc[\n",
    "            article_data['title'].str.lower().eq(y)\n",
    "        ].squeeze(),\n",
    "        plosData.loc[i, 'doi']\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same DOI, different titles\n",
    "x = plosData.loc[~sel_new_doi & sel_new_title, 'doi']\n",
    "for y in x:\n",
    "    print(\n",
    "        plosData.loc[plosData['doi'].eq(y), \"Title (URL items only)\"].squeeze(),\n",
    "        article_data.loc[article_data['doi'].explode().eq(y), 'title'].squeeze(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All done, now just mess around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.issn.str.replace('[^\\d]', '', regex=True).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.issn.str.replace('[^\\d]', '', regex=True).value_counts().reset_index().plot(loglog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.groupby('year').size().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with our 10 article sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois = pd.Series(\"\"\"\n",
    "    10.1371/journal.pone.0187219\n",
    "    10.1371/journal.pone.0059840\n",
    "    10.1371/journal.pone.0030837\n",
    "    10.1371/journal.pone.0118545\n",
    "    10.1371/journal.pone.0206678\n",
    "    10.1371/journal.pone.0143547\n",
    "    10.1371/journal.pone.0220751\n",
    "    10.1371/journal.pone.0107216\n",
    "    10.1371/journal.pone.0226761\n",
    "    10.1371/journal.pone.0193744\n",
    "\"\"\".split()).str.translate(\n",
    "    str.maketrans(string.ascii_lowercase, string.ascii_uppercase)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois[dois.isin(article_data.doi.explode())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
