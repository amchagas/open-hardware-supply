---
title: "combine_repeated_entries"
author: "Matias Andina"
date: "`r Sys.Date()`"
output: html_document
---
## RMarkdown to filter and combine repetitive entries of the 'allData.csv' file

```{r attach-libraries}
## clear workspace
# rm(list = ls())

library(tidyverse)#
library(here)
```


```{r load-csv}
allData <- read.csv(here("data","allData.csv"))
#names(allData)
```

## removing errors on title

There is a lot of garbage that needs almost manual cleaning with `gsub`, we might not need it if using [[:punct:]]

```{r}
## remove \n
allData$title <- gsub("\n"," ",allData$title)

## remove "\\&"
allData$title <- gsub(" \\&"," and",allData$title)
allData$title <- gsub("\\&","and",allData$title)

## "a modular path planning solution for wire + arc additive manufacturing"                                
## "a modular path planning solution for wire plus arc additive manufacturing" 
allData$title <- gsub(" + "," plus ",allData$title)

##  "a new paradigm for open robotics research and education with the c plus plus ooml"                                                                  ## "a new paradigm for open robotics research and education with the c++ ooml"   
allData$title <- gsub("\\+\\+", " plus plus", allData$title)

```


> [[:punct:]]Punctuation characters
> ;!"#$%&’()*+,-./:;<=>?@[]^_`{|}~



```{r sort-title}


allData <- allData %>%
  # put into lower case and remove punctuation
  mutate(title=tolower(title),
         title = str_replace_all(title,
                 pattern="[[:punct:]]",
                 replacement = " "),
         # remove all backticks
         # (for some reason some escape to punct)
         title = str_remove_all(title,
                               "`"),
         # remove extra spaces
         title = stringr::str_trim(str_squish(title)))

```


More manual cleaning 

> It's possible to check patterns using 

`filter(allData, str_detect(title, "bridging digital divide")) %>% pull(title)` 

or 

`filter(allData, str_detect(title, "your pattern here")) %>% pull(title)`

```{r clean-naming}

## "a low-cost platform for eye-tracking research: using pupil (c) in behavior analysis"                  
## "a low-cost platform for eye-tracking research: using pupil© in behavior analysis"
allData$title <- gsub("©","(c)",allData$title)

## substitute "µ" for "um"
allData$title <- gsub("µ","um",allData$title)

## substitute "®" for "(r)"
allData$title <- gsub("®","(r)",allData$title)


## "a real-time imaging system fo lumber strength prediction"
## "a real-time imaging system for lumber strength prediction"    
allData$title <- gsub("system fo lumber","system for lumber",allData$title)

## "a review of arduino board's, lilypad's \\and arduino shields"                                       
## "a review of arduino board's, lilypad's and arduino shields"  

## "a simple strategy for carboxylated mwnts as a metal-free electrosensor for anchoring the rhb c=n group"                                             
## "a simple strategy for carboxylated mwnts as a metal-free electrosensor for anchoring the rhb cn group" 
allData$title <- gsub("rhb c=n","rhb cn", allData$title)


## "afm nanœye – development of an education oriented high resolution profilometer"                          ## "afm nanoeye-development of an education oriented high resolution profilometer"  
allData$title <- gsub("œ","oe", allData$title)

## "an easily deployable system for indoor positioining using wi-fi network"                                   
## "an easily deployable system for indoor positioning using wi-fi network" 
allData$title <- gsub("positioining","positioning", allData$title)


## "an iot based service system as a research and educational platfor"                                                        
## "an iot based service system as a research and educational platform"  
allData$title <- gsub("and educational platfor","and educational platform",allData$title)

## "cost-effective n:1 firewall array via subnet-levcl load balancing by sdn/openflow switches"                               
## "cost-effective n:1 firewall array via subnet-level load balancing by sdn/openflow switches"  
## substitute "levcl" for "level"
allData$title <- gsub("levcl","level",allData$title)

## "cap floor - a flexible capacitive indoor localization system"                                                                    
## "capfloor - a flexible capacitive indoor localization system" 
## substitute "cap floor" for "capfloor"
allData$title <- gsub("cap floor","capfloor",allData$title)

## "deploying sdn in geant production network"
## "deploying sdn in géant production network" 

## "development of the gambiarradio educacional prototype: device for transmitting audio via fm radio waves based on raspberry pi"            
## "development of the gambiarrádio educational prototype: device for transmitting audio via fm radio waves based on raspberry pi"
#lowTitles <- gsub("á","a",lowTitles)
#lowTitles <- gsub("ó","o",lowTitles)
#lowTitles <- gsub("é","e",lowTitles)
# I think this does the same hopefully not introducing problems
allData$title <- stringi::stri_trans_general(allData$title,"Latin-ASCII")


## "differentiation between organic and non-organic apples using diffraction grating and image processing—a cost-effective approach"                    
## "differentiation between organic and non-organic apples using diffraction grating and image processinga cost-effective approach"   
allData$title <- gsub("processinga", "processing a", allData$title)

## "bridging digital divide: 'village wireless lan', a low cost network infrastructure solution for digital communication, information dissemination and education in rural bangladesh"                                
## "bridging digital divide:'village wireless lan', a low cost network infrastructure solution for digital communication, information dissemination \\and education in rural bangladesh"
allData$title <- gsub("divide:'village","divide: 'village",allData$title)

# on line has to be one word now
allData$title <- gsub("on line","online", allData$title)

## "implementation of vision-based autonomous mobile platform to control by a* algorithm"                                           ## "implementation of vision-based autonomous mobile platform to control by a∗ algorithm"
allData$title <- gsub("∗","*", allData$title)

## "livecoding the synthkit: littlebits as an embodied programming language"
## "livecoding the synthkit: little bits as an embodied programming language"
allData$title <- gsub("littlebits","little bits",allData$title)

## "modular open hardware omnidirectional platform for mobile robot research *" 
## "modular open hardware omnidirectional platform for mobile robot research" 
## "low comple*ity and low cost hardware sharing design of fast multistandard 2 d dct idct for image video coding"
allData$title <- gsub(" \\*", "", allData$title)
allData$title <- gsub("comple*ity","complexity",allData$title)

## "open labware: 3-d printing your own lab equipment" 
## "open labware: 3-d printing your own lab equipment (vol 13, e1002086, 2015)"
allData$title <- gsub(" vol 13 e1002086 2015","",allData$title)


## "opens hub: real-time data logging, connecting field sensors t o google sheets"
## "opens hub: real-time data logging, connecting field sensors t o google sheets"
allData$title <- gsub(" t o "," to ",allData$title)

## "self-sustainable monitoring station for f,xtreme environments (s3me2): design and validation" 
## "self-sustainable monitoring station for extreme environments (s3me2): design and validation" 
allData$title <- gsub("f,xtreme","extreme",allData$title)

## "the complementarity of openness: how makerbot leveraged thingiverse in 3dprinting"    
## "the complementarity of openness: how makerbot leveraged thingiverse in 3d printing"    
allData$title <- gsub("3dprinting","3d printing",allData$title)

## "towards secure elements for trusted transactions in blockchain and blochchain iot (biot) platforms. invited paper"   
## "towards secure elements for trusted transactions in blockchain and blochchain iot (biot) platforms"
allData$title <- gsub(" invited paper","", allData$title)

## "very high resolution crop surface models (csms) from uav-based stereo images for rice growth monitoring in northeast china" 
## "very high resolution crop surface models (csm) from uav-based stereo images for rice growth monitoring in northeast china" 
allData$title <- gsub("csms","csm",allData$title)

## "x4-mag: a low-cost open-source micro-quadrotor and its linux based controller" 
## "x4-mag: a low-cost open-source micro-quadrotor and its linux-based controller" 
allData$title <- gsub("linux-based","linux based",allData$title)

#### brought this one below on myself
## "openwsn \\and openmote: demo'ing a complete ecosystem for the industrial internet of things"
## "openwsn and openmote: demo'ing a complete ecosystem for the industrial internet of things" --> This one is fine now no need for intervention

## CHECK THESE ONES BELOW in MORE DETAIL BEFORE MERGE

## "research on detection of beef freshness parameters based on multi spectral diffuse reflectance method [基于多光谱漫反射的牛肉品质参数检测方法研究]" 
## "research on detection of beef freshness parameters based on multi spectral diffuse reflectance method"
# mind the space! in " 基于多光谱漫反射的牛肉品质参数检测方法研究"
allData$title <- gsub(" 基于多光谱漫反射的牛肉品质参数检测方法研究","", allData$title)


## "optimization and hardware implementation of image watermarking for low cost applications" 
## "optimization and hardware implementation of image and video watermarking for low-cost applications" 

## "peristaltic pump and detection system controlled by an arduino open source hardware developed and designed for application in a flow analysis system"
## "peristaltic pump and detection system controlled by an arduino open source hardware developed and designed for application in a flow analysis system [construção de uma bomba peristaltica e de um sistema de detecção utilizando um hardware de codigo fonte aberto ⇜arduino⇝ para analise em fluxo]"

## "archeorobotics. open robotic applications and extreme archeology"                                                       ## "archeorobotics. open robotic applications and extreme archeology [archeorobotics. applicazioni robotiche aperte e archeologia estrema]" 


## "arduino microcontroller processing for everyone! third edition barrett"                                                          
## "arduino microcontroller: processing for everyone"    

## "beach profiles: should the emery rod method be discarded?"    
## "beach profiles: should the emery rod method be discarded? [perfis de praia: deve o método das balizas de emery ser abandonado?]"  

# "big data in chemicals sector"
# "big data in the chemicals sector" 


## "designing low-cost hardware accelerators for ce devices"                                                           
## "designing low-cost hardware accelerators for ce devices [hardware matters]"   

## "design of an open source-based control platform for an underwater remotely operated vehicle"                                                         
## "design of an open source-based control platform for an underwater remotely operated vehicle [diseño de una plataforma de control basada en fuente abierta para un vehículo subacuático operado remotamente]"  

## "design and implementation of a modern automatic deformation monitoring system"                                                            
## "design and implementation of a modern automatic deformation monitoring system: towards an open source software platform for geodetical and geotechnical measurements" 

## "consideration of the versatility of the open prototype for educational nanosats cubesat design"                                                   
## "consideration of the versatility of the open prototype for educational nanosats cubesat design an overview of multiple uses and discussion of prospective future uses"  

## "a compact hardware implementation of sm3"                     ## "a compact hardware implementation of sm3 hash function"  

## "3d cardiac electrical activity model"                         ## "3d cardiac electrical activity model [3d model električne aktivnosti srca]"    

## "learning basic robotics and val ii programming with lego mindstorms robots"                                           
## "learning basic robotics and val ii programming with lego mindstorms robots [aprendizaje de robótica básica y programación en val ii usando robots lego mindstoms]"   

 
```

## Trying to remove duplicates

The distance matrix with `adist` takes forever, do not run!

```{r, eval=FALSE}
titles <- df$title
dist_mat <- adist(titles, titles)
```

```{r, eval=FALSE}
hist(unlist(dist_mat[lower.tri(dist_mat)]))
```


- clean Title -> done
- clean Journals 
- interaction title, journal, year
- flag that tells me whether duplicated
- remove duplicated
- only keep articles

# Same clean for the journals

Before clean, there are `r unique(allData$journal) %>% length()` journals.


<<<<<<< HEAD
```{r}
allData$journal <- gsub(" \\&|\\&"," and",allData$journal)
#allData$journal <- gsub("&", "and", allData$journal)

allData <- allData %>%
  # put into upper case and remove punctuation
  mutate(journal=toupper(journal),
         journal = str_replace_all(journal,
                 pattern="[[:punct:]]",
                 replacement = " "),
                  # remove extra spaces
         title = stringr::str_trim(str_squish(title)))

```


After clean, we have a few less unique journals `r unique(allData$journal) %>% length()`.

We will create a column identifying title, journal and year together.

```{r}
allData %>% 
  mutate(triple = paste(year, journal, title),
         duplicated = duplicated(triple)) -> allData

allData %>% count(duplicated)
```

- clean Title -> done
- clean Journals -> done
- interaction title, journal, year -> done
- flag that tells me whether duplicated -> done
-remove duplicated 
- only keep articles

```{r}
allData %>% filter(duplicated == FALSE,
                   str_detect(type, "[Aa]rticle")) %>% 
  mutate(doi = na_if(doi, "")) %>% 
  count(is.na(doi))
```

We still have duplicated papers...

```{r}
allData %>% filter(duplicated == FALSE,
                   str_detect(type, "[Aa]rticle")) %>% 
  mutate(doi = na_if(doi, "")) %>% 
  count(doi, sort = TRUE)
```

So basically we remove everything that is duplicated or is na.

```{r}
clean_data <- allData %>% 
  mutate(doi = forcats::fct_explicit_na(na_if(doi, ""))) %>%
  filter(duplicated == FALSE,
         str_detect(type, "[Aa]rticle"),
         !is.na(doi),
         !duplicated(doi)
        ) 

```


```{r}
=======
```{r}
allData$journal <- gsub(" \\&|\\&"," and",allData$journal)
#allData$journal <- gsub("&", "and", allData$journal)

allData <- allData %>%
  # put into upper case and remove punctuation
  mutate(journal=toupper(journal),
         journal = str_replace_all(journal,
                 pattern="[[:punct:]]",
                 replacement = " "),
                  # remove extra spaces
         title = stringr::str_trim(str_squish(title)))

```


After clean, we have a few less unique journals `r unique(allData$journal) %>% length()`.

We will create a column identifying title, journal and year together.

```{r}
allData %>% 
  mutate(triple = paste(year, journal, title),
         duplicated = duplicated(triple)) -> allData

allData %>% count(duplicated)
```

- clean Title -> done
- clean Journals -> done
- interaction title, journal, year -> done
- flag that tells me whether duplicated -> done
-remove duplicated 
- only keep articles

```{r}
allData %>% filter(duplicated == FALSE,
                   str_detect(type, "[Aa]rticle")) %>% 
  mutate(doi = na_if(doi, "")) %>% 
  count(is.na(doi))
```

We still have duplicated papers...

```{r}
allData %>% filter(duplicated == FALSE,
                   str_detect(type, "[Aa]rticle")) %>% 
  mutate(doi = na_if(doi, "")) %>% 
  count(doi, sort = TRUE)
```

So basically we remove everything that is duplicated or is na.

```{r}
clean_data <- allData %>% 
  mutate(doi = forcats::fct_explicit_na(na_if(doi, ""))) %>%
  filter(duplicated == FALSE,
         str_detect(type, "[Aa]rticle"),
         !is.na(doi),
         !duplicated(doi)
        ) 

```


```{r}
>>>>>>> 13ca04635802e68bf4109fce5164b5c3b5aaf5b3
doi_list <- clean_data  %>% pull(doi)
# safely get dois
# this has been done previously, data provided as .Rdata
#my_data <- purrr::map(doi_list, 
#  .f = purrr::safely(function(x) roadoi::oadoi_fetch(x,
#  .progress = "text",
#  email = "matiasandina@gmail.com")))

results <- purrr::map_df(my_data, "result")

urls <-  results %>% 
  mutate(
  urls = purrr::map(best_oa_location, "url") %>% 
    purrr::map_if(purrr::is_empty, ~ NA_character_) %>% 
    purrr::flatten_chr()
) %>%
  .$urls

```


```{r}
results %>%
  group_by(is_oa) %>%
  summarise(Articles = n()) %>%
  ggplot(aes(is_oa, Articles)) +
  geom_col()+
  geom_text(aes(y=Articles - 80, label=Articles), color="white", size=12)+
  labs(x="Open Access",
       title="Open Access papers in dataset")
```

<<<<<<< HEAD


## Reading and downloading

```{r}
clean_data <- readRDS("clean_data.RData")
clean_data <- purrr::map_df(clean_data, "result")
names(clean_data)
```

```{r}
if (!dir.exists("pdf")) { dir.create("pdf")}
```


```{r}
# mutate clean data so that we get the url
clean_data <-  clean_data %>% 
  mutate(
  urls = purrr::map(best_oa_location, "url") %>% 
    purrr::map_if(purrr::is_empty, ~ NA_character_) %>% 
    purrr::flatten_chr(),
  fake_id = str_pad(as.character(1:n()), width = 3, pad = 0),
  pdf_name = file.path("pdf", paste0(fake_id, ".pdf"))
  
) 

clean_data %>% filter(!is.na(urls)) -> to_download

for (file in seq_along(1:nrow(to_download))) {
  if (as.numeric(to_download$fake_id[file]) <= 915) { # do nothing
  } else {
        safely(download.file(url = to_download$urls[file],
                       destfile = to_download$pdf_name[file]))
    }
}

# errors found
# 'http://dogadergi.ksu.edu.tr/tr/download/article-file/480622' -> downloaded as 295
# 'http://manuscript.elsevier.com/S0921889016307837/pdf/S0921889016307837.pdf' -> manuscript not available, downloaded as 327
# http://www.ejmste.com/pdf-62173-11713?filename=Framework for Reducing.pdf -> cannot open URL, downloaded as 431.pdf
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3882465/pdf/ -> page doesn't exist, downloaded as 601
# http://www.ent.mrt.ac.lk/iml/paperbase/TRO Collection/TRO/2006/june/9.pdf -> download as 777.pdf 
# http://www.cjmenet.com.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=5113 -> weird pdf went to 915
```

For some reason, some papers fail downloading a 10 kB paper that can't be opened... Fixing that

```{r}
pdfs <- list.files("pdf/", full.names = T)
sizes <- file.size(pdfs)
# 300000 is 300 kB
# these files are likely wrong...
wrong_files <- which(sizes < 300000)
files_to_check <- to_download[wrong_files, ] %>% 
  mutate(person = cut(1:length(fake_id),
                      breaks = 5, labels = LETTERS[1:5]))
```


```{r}
# write to files
files_to_check %>% 
  select(doi, title, fake_id, pdf_name, person) %>%
  write_csv("files_to_check.csv")
```



```{r}
keywords <- c(
"[Aa]vailable code",
"[Cc]ode is available",
"github|GitHub|Github",
"[Ss]upplemental",
"[Ss]hared",
"download",
"[Gg]itlab",
"[Rr]epositor",
"[Bb]ill of materials",
"[Cc]omponents list")

library(pdftools)


read_pdfs <- function(file) {
  txt <- pdf_text(file)
  txt %>% readr::read_lines() %>%
  stringr::str_squish() -> txt
  
  qq <- sapply(keywords,
               function(tt) 
                 str_detect(string=txt,
                                    pattern =tt) %>%
                 any()
  )
  
  return(list(file = file,
              txt = txt,
              matches = qq))
}

```



```{r, warning=FALSE, message=FALSE}
# this will throw a bunch of errors
pdf_results <- list()

pdf_results <-
purrr::map(pdfs, purrr::safely(read_pdfs))

```



Get the matches

```{r}
M <- lapply(1:length(pdf_results),  
            function(element) pdf_results[[element]]$result$matches)
     

names(M) <- pdfs

```


```{r}
do.call(rbind, M) %>%
  as_tibble() %>%
  mutate_all(as.numeric) %>%
  summarise_all(sum) %>%
  pivot_longer(everything())
```

=======
>>>>>>> 13ca04635802e68bf4109fce5164b5c3b5aaf5b3
